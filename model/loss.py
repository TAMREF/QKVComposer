from omegaconf import DictConfig
import torch.nn as nn
import torch
import torch.nn.functional as F

class GenericLoss:
    def __init__(self, cfg):
        super(GenericLoss, self).__init__()
        self.reduction = cfg.train.reduction
        self.label_smoothing = cfg.train.label_smooth
        self.vocab_size = cfg.model.vocab_size
        self.ignore_index = -100

    def __call__(self, logits, target):
        """
        Args:
            input: [B * T, V]
            target: [B * T]
        Returns:
            cross entropy: [1]
        """
        mask = (target == self.ignore_index).unsqueeze(-1)
        q = F.one_hot(target.long(), self.vocab_size).type(torch.float32)
        u = 1.0 / self.vocab_size
        q_prime = (1.0 - self.label_smoothing) * q + self.label_smoothing * u
        q_prime = q_prime.masked_fill(mask, 0)

        ce = self.cross_entropy_with_logits(q_prime, logits)
        if self.reduction == 'mean':
            lengths = torch.sum(target != self.ignore_index)
            return ce.sum() / lengths
        elif self.reduction == 'sum':
            return ce.sum()
        else:
            raise NotImplementedError

    def cross_entropy_with_logits(self, p, q):
        return -torch.sum(p * (q - q.logsumexp(dim=-1, keepdim=True)), dim=-1)


class _Metric(torch.nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, input: torch.Tensor, target: torch.Tensor):
        raise NotImplementedError()

class Accuracy(_Metric):
    def __init__(self):
        super().__init__()

    def forward(self, input: torch.Tensor, target: torch.Tensor):
        """
        :param input: [B, L]
        :param target: [B, L]
        :return:
        """
        bool_acc = input.long() == target.long()
        return bool_acc.sum().to(torch.float) / bool_acc.numel()

class CategoricalAccuracy(Accuracy):
    def __init__(self):
        super().__init__()

    def forward(self, logits: torch.Tensor, target: torch.Tensor):
        """
        :param input: [B, T, V]
        :param target: [B, T]
        :return:
        """
        input = logits.softmax(-1)
        categorical_input = input.argmax(-1)
        return super().forward(categorical_input, target)