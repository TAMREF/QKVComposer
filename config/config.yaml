model:
  d_ff: 2048
  d_hidden: 2048
  d_embed: 2048
  dropout: 0.1
  data_len: 256
  num_time_token: 500
  n_head: 4
  num_layers: 12
  num_tokens: 385
  padding_idx: 384
  sinusoid_const: 0.1
  use_temporal_encoding: True

log:
  checkpoint_dir: 'checkpoints'
  log_dir: 'logs'
  name: 'test'
  trace: False
  version: 0

data:
  datamidi_dir: 'dataset/midi'
  dataset_path: ''
  make_data_tensor: False
  max_len: 1000000
  datatensor_dir: 'dataset/time_token_tensor'
  num_workers: 0
  datamode: 'time_note_vel'
  #Set to 'full' if you want to use full dataset
  data_size: 'full'

train:
  accumulate: 1
  batch_size: 4
  betas: [0.5, 0.9]
  epochs: 10000
  fast_dev_run: False
  gpus: 1
  lr: 0.00003
  optim: 'adam'
  resume: 'outputs\2021-02-25\04-26-20\checkpoints\ckpt-epoch=0106-val_loss=3.3945.ckpt'
  save_top_k: 3
  steps: 100
  time_loss_mode: 'one_hot'
  time_loss_mul: 1
  valid_split: 2048

inference:
  length: 100
  condition_pt: 'dataset\time_token_tensor\Bach Prelude No 1 (Ave Maria)time_token_tensor.pt'
  condition_length: 10
  checkpoint_path: 'outputs\2021-02-25\18-09-44\checkpoints\ckpt-epoch=0116-val_loss=3.0052.ckpt'
  sample: False
  save_path: 'samples'
  sample_mode: 'OneHotCategorical'
  zero_threshold: 0.5
  one_hot_smooth: 1
  beam_search:
    k: 20